# case-nyc-taxis
Projeto de ingestÃ£o e ETL com dados pÃºblicos sobre tÃ¡xis de Nova York (NYC Taxis).

## Sobre o projeto

Este projeto realiza a ingestÃ£o de dados brutos, seguida por um processo de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga) para limpar, transformar e preparar os dados para anÃ¡lise. O desenvolvimento foi feito utilizando o ambiente Databricks.

## ðŸ› ï¸ PrÃ©-requisitos

Antes de comeÃ§ar, garanta que vocÃª tenha os seguintes prÃ©-requisitos:
*   Python 3.8+
*   Acesso a um ambiente Databricks ou outro que rode o Spark.
*   Um bucket na AWS S3 e as devidas credenciais de acesso (Access Key ID, Secret Access Key e Nome do Bucket).

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

1.  **Clone o repositÃ³rio:**
    ```bash
    git clone https://github.com/thimac/case-nyc-taxis.git
    cd case-nyc-taxis
    ```

2.  **Instale as dependÃªncias:**
    Para instalar as bibliotecas necessÃ¡rias, execute o comando abaixo:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure as variÃ¡veis de ambiente:**
    Este projeto precisa de credenciais para se conectar ao seu bucket no S3. Crie um arquivo chamado `.env` na raiz do projeto.

    Copie o conteÃºdo do exemplo abaixo para o seu arquivo `.env` e substitua os valores pelas suas credenciais:
    ```
    AWS_ACCESS_KEY_ID="SUA_AWS_ACCESS_KEY_ID"
    AWS_SECRET_ACCESS_KEY="SUA_AWS_SECRET_ACCESS_KEY"
    S3_BUCKET="SEU_S3_BUCKET"
    ```
    > **AtenÃ§Ã£o:** Arquivo `.env` jÃ¡ estÃ¡ no inserido no `.gitignore`.

## ðŸš€ Como Executar

A execuÃ§Ã£o do projeto pode variar dependendo da sua versÃ£o do Databricks ou outro ambiente.

### Ambiente Databricks (VersÃ£o Paga)
Se vocÃª estiver utilizando uma versÃ£o paga do Databricks, que possui suporte Ã  funcionalidade `dbutils.notebook.run`, a execuÃ§Ã£o pode ser centralizada.
*   Execute o notebook `main.ipynb`. Ele foi projetado para orquestrar e executar sequencialmente os notebooks de ingestÃ£o e ETL.

### Ambiente Databricks (VersÃ£o Community) ou Outro Ambiente
Caso esteja utilizando a versÃ£o Community do Databricks (que nÃ£o suporta `dbutils.notebook.run`) ou executando os scripts em outro ambiente, serÃ¡ necessÃ¡rio executar cada notebook separadamente, na seguinte ordem:
1.  **Notebook de IngestÃ£o:** Execute primeiro o script responsÃ¡vel pela ingestÃ£o dos dados.
2.  **Notebook de ETL:** ApÃ³s a conclusÃ£o da ingestÃ£o, execute o script que realiza o processo de ETL.

## ðŸ“Š AnÃ¡lise dos Dados

Com os dados disponibilizados para consuma em uma tabela externa, eles estÃ£o prontos para serem explorados.

### 1. AnÃ¡lise via Notebook
*   O notebook `analise_consumo.ipynb` contÃ©m as respostas para as questÃµes do teste, bem como outras anÃ¡lises exploratÃ³rias. Nele, vocÃª encontrarÃ¡ os principais insights gerados a partir dos dados processados.

    **ObservaÃ§Ãµes sobre a Qualidade dos Dados:**

    Durante a anÃ¡lise, foram identificados alguns pontos que merecem atenÃ§Ã£o e um aprofundamento futuro para garantir a total acuracidade dos resultados:

    *   **Tratamento de Nulos:** A Ãºnica transformaÃ§Ã£o de limpeza aplicada durante a transformaÃ§Ã£o  foi a substituiÃ§Ã£o de valores nulos da coluna `passenger_count` pela **mediana** da respectiva coluna.
    *   **DistribuiÃ§Ã£o de Datas:** Foi observado que a distribuiÃ§Ã£o de meses nos dados parece ser maior do que o range de dados que foi ingerido, o que pode indicar um problema na fonte.
    *   **Valores Inconsistentes:**
        *   A coluna `total_amount` apresenta valores negativos, o que Ã© atÃ­pico para uma cobranÃ§a de corrida e precisa ser investigado.
        *   A coluna `passenger_count` contÃ©m valores muito acima da capacidade de um tÃ¡xi convencional (ex: mais de 6 passageiros), sugerindo erros de digitaÃ§Ã£o ou de sistema.
    *   **Engenharia de Features:** Para anÃ¡lises mais detalhadas, seria benÃ©fico criar novas colunas a partir das existentes. Por exemplo, extrair a data e a hora separadamente da coluna `pickup_datetime` pode facilitar anÃ¡lises de sazonalidade diÃ¡ria ou semanal. A Ãºnica coluna criada neste projeto foi `mes_pickup`, com o propÃ³sito especÃ­fico de particionar a tabela final para otimizar as consultas.


### 2. Consulta via Tabela Externa (SQL)
O processo de ETL tambÃ©m cria uma tabela externa no metastore do Databricks. Essa tabela aponta diretamente para os dados processados e armazenados no formato Delta Lake em seu bucket S3.

Isso permite que vocÃª e sua equipe consultem os dados de forma eficiente usando Spark SQL.

> **Importante: Configurando o Acesso ao S3**
> Para que o Databricks consiga ler os dados da tabela externa, a sessÃ£o do Spark precisa ser configurada com as mesmas credenciais do S3 que foram usadas para escrever os dados. Se vocÃª estiver em um novo notebook ou em uma nova sessÃ£o, precisarÃ¡ configurar o acesso ao S3 antes de executar as consultas SQL.
>
> ```python
> # Exemplo de como configurar a sessÃ£o do Spark em um notebook.
> AWS_ACCESS_KEY_ID = "SUA_AWS_ACCESS_KEY_ID"
> AWS_SECRET_ACCESS_KEY = "SUA_AWS_SECRET_ACCESS_KEY"
> 
> spark.conf.set("spark.hadoop.fs.s3a.access.key", AWS_ACCESS_KEY_ID)
> spark.conf.set("spark.hadoop.fs.s3a.secret.key", AWS_SECRET_ACCESS_KEY)
> spark.conf.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
> ```

Uma vez configurado, basta acessar normalmente a tabela `taxi_consumo` via Spark SQL.
