{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea1445b-8b2d-4237-98ec-e01b3d7d18ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# Configuração de logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def configurar_ambiente():\n",
    "    if not load_dotenv('.env'):\n",
    "        logger.warning(\"Arquivo .env não encontrado.\")\n",
    "\n",
    "    # Verifica e solicita configurações faltantes\n",
    "    verificar_configuracao('AWS_ACCESS_KEY_ID', \"AWS Access Key ID\")\n",
    "    verificar_configuracao('AWS_SECRET_ACCESS_KEY', \"AWS Secret Access Key\")\n",
    "    verificar_configuracao('S3_BUCKET', \"Nome do Bucket S3\")\n",
    "\n",
    "def verificar_configuracao(variavel: str, descricao: str):\n",
    "    if not os.getenv(variavel):\n",
    "        print(f\"\\nConfiguração não encontrada: {descricao}\")\n",
    "        valor = input(f\"Por favor, informe o valor para {descricao}: \").strip()\n",
    "        \n",
    "        os.environ[variavel] =  valor\n",
    "        logger.info(f\"Configuração {variavel} salva na seção atual\")\n",
    "\n",
    "def criar_spark_session():\n",
    "    \"\"\"Cria e configura uma sessão Spark com acesso ao S3.\"\"\"\n",
    "    # Configura as credenciais AWS a partir de variáveis de ambiente\n",
    "    logger.info(f\"Criando Seção Spark...\")\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"ETL-Yellow-Taxis\") \\\n",
    "            .config(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\")) \\\n",
    "            .config(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "            .config(\"fs.s3a.endpoint\", \"s3.sa-east-1.amazonaws.com\") \\\n",
    "            .config(\"fs.s3a.region\", \"sa-east-1\") \\\n",
    "            .getOrCreate()\n",
    "        logger.info(f\"Seção Spark criada com Sucesso!\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha ao criar a sessão Spark: {e}\")\n",
    "        raise\n",
    "\n",
    "def carregar_dados(spark: SparkSession, s3_bucket: str):\n",
    "    logger.info(f\"Lendo dados do bucket {s3_bucket}\")\n",
    "    caminhos = [\n",
    "        f\"s3a://{s3_bucket}/raw/yellow/yellow_tripdata_2023-01.parquet\",\n",
    "        f\"s3a://{s3_bucket}/raw/yellow/yellow_tripdata_2023-02.parquet\",\n",
    "        f\"s3a://{s3_bucket}/raw/yellow/yellow_tripdata_2023-03.parquet\",\n",
    "        f\"s3a://{s3_bucket}/raw/yellow/yellow_tripdata_2023-04.parquet\",\n",
    "        f\"s3a://{s3_bucket}/raw/yellow/yellow_tripdata_2023-05.parquet\",\n",
    "        f\"s3a://{s3_bucket}/raw/green/green_tripdata_2023-01.parquet\",\n",
    "        f\"s3a://{s3_bucket}/raw/green/green_tripdata_2023-02.parquet\",\n",
    "        f\"s3a://{s3_bucket}/raw/green/green_tripdata_2023-03.parquet\",\n",
    "        f\"s3a://{s3_bucket}/raw/green/green_tripdata_2023-04.parquet\",\n",
    "        f\"s3a://{s3_bucket}/raw/green/green_tripdata_2023-05.parquet\"\n",
    "    ]   \n",
    "    try:\n",
    "        dfs = [ler_e_padronizar(caminho, spark) for caminho in caminhos]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha ao ler os dados do S3: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Combinar todos os DataFrames\n",
    "    df = dfs[0]\n",
    "    for temp_df in dfs[1:]:\n",
    "        df = df.union(temp_df)\n",
    "\n",
    "    logger.info(f\"Dados lidos com Sucesso\")\n",
    "    return df\n",
    "\n",
    "def ler_e_padronizar(caminho: str, spark: SparkSession):\n",
    "    df = spark.read.option(\"mergeSchema\", \"true\").parquet(caminho)\n",
    "    colunas_necessarias = [\n",
    "        \"VendorID\", \n",
    "        \"passenger_count\", \n",
    "        \"total_amount\",\n",
    "        \"pickup_datetime\", \n",
    "        \"dropoff_datetime\",\n",
    "        \"taxi_type\"\n",
    "    ]\n",
    "    #extrai tipo do taxi\n",
    "    tipo_taxi = re.search(r'/(green|yellow)/', caminho).group(1)\n",
    "    \n",
    "    # Passenger_count é double em janeiro e long nos demais, por isso é necessário converter cada um separadamente.\n",
    "    # Demais colunas ele consegue inferir o schema e os tipos normalmente.\n",
    "    if \"passenger_count\" in df.columns:\n",
    "        df = df.withColumn(\"passenger_count\", df[\"passenger_count\"].cast(\"long\"))\n",
    "\n",
    "    df = df.withColumn(\"taxi_type\", lit(tipo_taxi))\n",
    "\n",
    "    #renomeia as colunas que são diferentes de acardo com o tipo de taxi\n",
    "    if tipo_taxi == \"yellow\":\n",
    "        df = df.withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\") \\\n",
    "               .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "    else:\n",
    "        df = df.withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\") \\\n",
    "               .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "\n",
    "    return df.select(colunas_necessarias)\n",
    "\n",
    "def corrigindo_nulo(df):\n",
    "    print(\"\\n=== Valores Nulos ===\")\n",
    "    for c in df.columns:\n",
    "        null_count = df.where(col(c).isNull()).count()\n",
    "        print(f\"{c}: {null_count} valores ausentes ({null_count/df.count()*100:.2f}%)\")\n",
    "\n",
    "\n",
    "    # Calcular mediana APENAS para passenger_count\n",
    "    mediana_passenger = df.approxQuantile(\"passenger_count\", [0.5], 0.01)[0]\n",
    "\n",
    "    logger.info(f\"Corrigindo valores nulos da coluna passenger_count com a mediana {mediana_passenger}\")\n",
    "    # Preencher NULOS apenas nessa coluna\n",
    "    df_preenchido = df.fillna({\"passenger_count\": mediana_passenger})\n",
    "\n",
    "    return df_preenchido\n",
    "\n",
    "def salva_s3_clean(bucket, df):\n",
    "    logger.info(f\"Salvando dados transformados...\")\n",
    "\n",
    "    #Salva a versão limpa\n",
    "    clean_path = f\"s3a://{bucket}/clean/\"\n",
    "\n",
    "    try:\n",
    "        df.write \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .parquet(clean_path)\n",
    "        logger.info(f\"Dados salvos em: {clean_path} com Sucesso!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha ao escrever dados transformados: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def salva_s3_consumo(bucket, df):\n",
    "    logger.info(f\"Salvando tabela particionada para consumo...\")\n",
    "\n",
    "    #coluna criada para particionar a tabela de consumo\n",
    "    df = df.withColumn(\"mes_pickup\", month(\"pickup_datetime\"))\n",
    "\n",
    "    # Cria tabela particionada para consumo\n",
    "    try:\n",
    "        df.write \\\n",
    "            .partitionBy(\"mes_pickup\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"path\", f\"s3a://{bucket}/tables/taxi_consumo\") \\\n",
    "            .saveAsTable(\"taxi_consumo\")\n",
    "\n",
    "        logger.info(f\"Tabela taxi_consumo salva  com Sucesso!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha ao escrever tabela para consumo: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def principal():\n",
    "    # Configura ambiente\n",
    "    configurar_ambiente()\n",
    "\n",
    "    # Cria seção Spark\n",
    "    spark  = criar_spark_session()\n",
    "    \n",
    "    df = carregar_dados(spark, os.getenv(\"S3_BUCKET\"))\n",
    "    df_clean = corrigindo_nulo(df)\n",
    "    salva_s3_clean(os.getenv(\"S3_BUCKET\"), df_clean)\n",
    "    salva_s3_consumo(os.getenv(\"S3_BUCKET\"), df_clean)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    principal()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_transformacao",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}